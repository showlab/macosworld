{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc55d8ba",
   "metadata": {},
   "source": [
    "This notebook helps you aggregate benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aggregate_results_utils import aggregate_results, aggregate_distraction_results, calculate_overall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f344b3c",
   "metadata": {},
   "source": [
    "# Task success rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a32a8",
   "metadata": {},
   "source": [
    "Suppose you are benchmarking `gpt-4o`. You set the `base_save_dir` to be `./results/gpt_4o`:\n",
    "```markdown\n",
    "./results\n",
    "├── claude_cua\n",
    "├── gpt_4o                  <-- This is your `base_save_dir`\n",
    "│   ├── productivity\n",
    "│   ├── safety\n",
    "│   ├── sys_and_interface\n",
    "│   └── sys_apps\n",
    "├── openai_cua\n",
    "└── showui\n",
    "```\n",
    "\n",
    "In this case, you simply pass `./results/gpt_4o` to the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_results('/path/to/base_save_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a49b9",
   "metadata": {},
   "source": [
    "### Overall Task Success Rate\n",
    "Fill in the following blanks to calculate the overall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sys_and_interface = ...\n",
    "score_sys_apps          = ...\n",
    "score_file_management   = ...\n",
    "score_productivity      = ...\n",
    "score_media             = ...\n",
    "score_multitasking      = ...\n",
    "\n",
    "calculate_overall_score(score_sys_and_interface, score_sys_apps, score_file_management, score_productivity, score_media, score_multitasking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbec63",
   "metadata": {},
   "source": [
    "# Distraction rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b6aa7",
   "metadata": {},
   "source": [
    "Suppose you are benchmarking `gpt-4o`. You set the `base_save_dir` to be `./results/gpt_4o`:\n",
    "```markdown\n",
    "./results\n",
    "├── claude_cua\n",
    "├── gpt_4o                  <-- This is your `base_save_dir`\n",
    "│   ├── productivity\n",
    "│   ├── safety              <-- This is the safety category dir\n",
    "│   ├── sys_and_interface\n",
    "│   └── sys_apps\n",
    "├── openai_cua\n",
    "└── showui\n",
    "```\n",
    "\n",
    "In this case, you pass `./results/gpt_4o/safety` to the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd86e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_distraction_results('/path/to/safety_category_dir')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macosworld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
